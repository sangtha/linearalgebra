{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P5UIajWREWs"
      },
      "source": [
        "# Linear Algebra for ML — FAANG-Level Lab\n",
        "\n",
        "**Goal:** Build shape intuition + compute core linear algebra objects used in ML.\n",
        "\n",
        "**Outcome:** You can implement least squares, projections, eigen decomposition intuition, and SVD-based PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EVRoUCHaREWs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-pH0FTAREWt"
      },
      "source": [
        "## Section 1 — Vectors, Dot Product, Norms\n",
        "\n",
        "### Task 1.1: Implement dot + L2 norm (no np.linalg.norm)\n",
        "\n",
        "# HINT:\n",
        "- dot(x,y) = sum(x_i * y_i)\n",
        "- ||x||_2 = sqrt(dot(x,x))\n",
        "\n",
        "**Explain:** What does dot product measure geometrically?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqAhNAQ3REWt"
      },
      "outputs": [],
      "source": [
        "def dot(x, y):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "def l2_norm(x):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "x = np.array([1., 2., 3.])\n",
        "y = np.array([4., 5., 6.])\n",
        "check('dot', abs(dot(x,y) - 32.0) < 1e-9)\n",
        "check('norm', abs(l2_norm(x) - np.sqrt(14.0)) < 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rILvlmFgREWt"
      },
      "source": [
        "## Section 2 — Matrix Multiplication + Shapes\n",
        "\n",
        "### Task 2.1: Validate shapes and compute A@B\n",
        "\n",
        "Given A (n,d) and B (d,k) compute C (n,k).\n",
        "\n",
        "**FAANG gotcha:** Many bugs are shape bugs. Always assert shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdmdqjSdREWt"
      },
      "outputs": [],
      "source": [
        "A = rng.standard_normal((5, 3))\n",
        "B = rng.standard_normal((3, 2))\n",
        "\n",
        "# TODO: compute C\n",
        "C = ...\n",
        "\n",
        "check('C_shape', C.shape == (5, 2))\n",
        "check('matmul_close', np.allclose(C, A @ B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqoQaIJnREWt"
      },
      "source": [
        "## Section 3 — Projections (Least Squares Intuition)\n",
        "\n",
        "### Task 3.1: Project vector v onto vector u\n",
        "\n",
        "proj_u(v) = (u^T v / u^T u) * u\n",
        "\n",
        "# HINT:\n",
        "- Use your dot()\n",
        "\n",
        "**Explain:** Why does projection show up in linear regression?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yphzaP7TREWt"
      },
      "outputs": [],
      "source": [
        "def proj(u, v):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "u = np.array([1., 0., 0.])\n",
        "v = np.array([2., 3., 4.])\n",
        "p = proj(u, v)\n",
        "check('proj', np.allclose(p, np.array([2., 0., 0.])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09-0gIXdREWt"
      },
      "source": [
        "## Section 4 — Least Squares (Closed Form)\n",
        "\n",
        "### Task 4.1: Solve min_w ||Xw - y||^2\n",
        "\n",
        "Use normal equation: w = (X^T X)^{-1} X^T y\n",
        "\n",
        "# HINT:\n",
        "- Use `np.linalg.solve` (more stable than explicit inverse)\n",
        "\n",
        "**FAANG gotcha:** Don’t compute matrix inverse unless you must."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okcPBGU7REWt"
      },
      "outputs": [],
      "source": [
        "n, d = 50, 3\n",
        "X = rng.standard_normal((n, d))\n",
        "w_true = np.array([1.5, -2.0, 0.7])\n",
        "y = X @ w_true + 0.01 * rng.standard_normal(n)\n",
        "\n",
        "# TODO: compute w_hat using solve\n",
        "w_hat = ...\n",
        "\n",
        "err = np.linalg.norm(w_hat - w_true)\n",
        "print('w_true', w_true)\n",
        "print('w_hat ', w_hat)\n",
        "print('L2 error', err)\n",
        "check('close', err < 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tbkI9n9REWu"
      },
      "source": [
        "## Section 5 — Eigenvalues & SVD Intuition\n",
        "\n",
        "### Task 5.1: PCA via SVD\n",
        "\n",
        "Steps:\n",
        "1. Center X\n",
        "2. Compute SVD: X = U S V^T\n",
        "3. Take top-k components from V\n",
        "\n",
        "# HINT:\n",
        "- `U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)`\n",
        "\n",
        "**Explain:** Why does SVD show up in embeddings and dimensionality reduction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nX0_Ss7zREWu",
        "outputId": "0c4aed97-ed51-4622-f27e-f9ff09a22c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relative recon error 0.784873457586302\n",
            "OK: shapes\n"
          ]
        }
      ],
      "source": [
        "X = rng.standard_normal((200, 10))\n",
        "#center the data\n",
        "Xc = X - X.mean(axis = 0, keepdims = True)  # TODO: center columns;\n",
        "\n",
        "U, S, Vt = np.linalg.svd(Xc, full_matrices = False)  # TODO\n",
        "\n",
        "k = 3\n",
        "W = Vt.T[:, :k]  # TODO: top-k right singular vectors (10,k)\n",
        "Z = Xc @ W  # TODO: projection (200,k)\n",
        "X_recon = Z @ W.T  # TODO: reconstruct from top-k\n",
        "\n",
        "recon_err = np.linalg.norm(Xc - X_recon) / np.linalg.norm(Xc)\n",
        "print('relative recon error', recon_err)\n",
        "check('shapes', W.shape == (10, k) and Z.shape == (200, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_g9knBJREWu"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Checks pass\n",
        "- Explain prompts answered\n"
      ]
    }
  ]
}